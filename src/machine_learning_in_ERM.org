#+TITLE: æœºå™¨å­¦ä¹ åœ¨ä¼ä¸šé£é™©ç®¡ç†ä¸­çš„åº”ç”¨ä¸¾ä¾‹
#+filetags: :python:
#+HUGO_BASE_DIR: ../
#+HUGO_SECTION: posts
#+DATE: <2022-03-30 Wed>
#+BIBLIOGRAPHY: ref.bib
#+PROPERTY: header-args:jupyter-python :tangle ~/code/erm/main.py
* å‰è¨€
#+begin_src emacs-lisp :exports results :results file
(setq xkcd-cache-dir "/Users/dcy/Code/ernest/static/images/xkcd/")
;; (xkcd-get 1838)
(concat xkcd-cache-dir "1838.png")
#+end_src

#+RESULTS:
[[file:/Users/dcy/Code/ernest/static/images/xkcd/1838.png]]


ä»€ä¹ˆæ˜¯å­¦ä¹ ï¼Ÿ [[https://zh.wikipedia.org/wiki/%E5%AD%A6%E4%B9%A0][ç»´åŸºç™¾ç§‘]]ä¸Šè¯´å­¦ä¹ æ˜¯è·å¾—æ–°çš„ç†è§£ã€çŸ¥è¯†ã€è¡Œä¸ºã€æŠ€èƒ½ã€ä»·å€¼è§‚ã€æ€åº¦å’Œåå¥½çš„è¿‡ç¨‹ã€‚åœ¨è®¡ç®—æŠ€æœ¯å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œè®©æœºå™¨å»åˆ©ç”¨ç®—æ³•å’Œç®—åŠ›å»â€œå­¦ä¹ â€ã€æ¨ç†ã€å†³ç­–ï¼Œå°±æ˜¯æœºå™¨å­¦ä¹ ã€‚
æœºå™¨å­¦ä¹ æŒ‰ç…§å­¦ä¹ çš„æ–¹å¼å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç§ï¼Œä½†ä¹Ÿä¸ç»å¯¹ï¼Œå­˜åœ¨åŠç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿™ç§éš¾ä»¥å½’ç±»çš„æ–¹å¼ï¼Œéƒ¨åˆ†ç®—æ³•ä¹Ÿå¯ä»¥æ¨ªè·¨å‡ ç§åˆ†ç±»ï¼š
#+CAPTION: ä¸€äº›å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•
[[./lib/mathworks.svg]]

æœºå™¨å­¦ä¹ æ·±ç©¶çš„è¯ï¼Œéœ€è¦å­¦ä¹ å¾ˆå¤šæ•°å­¦å’Œè®¡ç®—æœºã€‚ä½†æ˜¯å·¥ä¸šç•Œå°†å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•å°è£…åœ°å¾ˆå¥½ï¼ˆpytorch, scikit-learnï¼‰ï¼Œå‡ è¡Œä»£ç å°±å¯ä»¥å®ç°ä¸€ä¸ªæ¨¡å‹ï¼Œ +ä¹‹åä¾¿æ˜¯æ¼«æ— æ­¢å¢ƒåœ°è°ƒå‚+ ã€‚

æœ¬æ–‡ä¸»è¦å‚è€ƒäº†[cite/text:@scikit-learn]çš„æ–‡æ¡£ï¼Œåœ¨ç¼–ç è¿‡ç¨‹ä¸­é˜…è¯»æ–‡æ¡£æ˜¯æœ‰å¸®åŠ©çš„ã€‚

* æœºå™¨å­¦ä¹ åœ¨ä¼ä¸šé£é™©ç®¡ç†ä¸­çš„åº”ç”¨
[cite/text:@mai2019deep] åˆ©ç”¨ CNN é¢„æµ‹ä¼ä¸šç ´äº§ï¼Œåœ¨å¤„ç†æ–‡æœ¬æ•°æ®æ—¶åˆ©ç”¨ word embedding é‡åŒ–ï¼ŒAUC æ›²çº¿å¦‚å›¾
[[https://ars.els-cdn.com/content/image/1-s2.0-S0377221718308774-gr5.jpg]]

[cite/text:@golbayani2020comparative]
ä½¿ç”¨å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºå’Œå¤šå±‚æ„ŸçŸ¥å™¨åº”ç”¨äºç›¸åŒçš„æ•°æ®é›†ï¼Œé¢„æµ‹å…¬å¸æœªæ¥è¯„çº§ã€‚ä»–ä»¬ç»Ÿè®¡äº†æœºå™¨å­¦ä¹ åœ¨å€ºåˆ¸è¯„çº§å’Œå…¬å¸ä¿¡ç”¨è¯„çº§æ–¹é¢çš„æ–‡ç« ï¼Œå¾ˆå¤šè®¤ä¸º SVM å’Œç¥ç»ç½‘ç»œæ˜¯æ¯”è¾ƒå‡†ç¡®çš„ã€‚ä½†æ˜¯ä»–ä»¬ä½¿ç”¨ Notches Distance æ¥å¯¹æœºå™¨å­¦ä¹ ç»©æ•ˆæ¥æ‰“åˆ†ï¼Œè®¤ä¸ºåŸºäºå†³ç­–æ ‘çš„ä¸¤ç§æ–¹æ³•æ›´æœ‰æ•ˆã€‚

[cite/text:@kellner2022opening] åˆ©ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹è¿çº¦æŸå¤± Loss Given Default
å°†ä¼ ç»Ÿçš„åˆ†ä½æ•°å›å½’çš„å›å½’å…ƒä½œä¸ºç¬¬ä¸€å±‚ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œæ­ç¤ºå…¶ä¸­çš„éçº¿æ€§å…³ç³»ï¼Œæ¯”å¦‚äº¤å‰é¡¹åŠå…¶ä»–éçº¿æ€§å…³ç³»ï¼Œç¥ç»ç½‘ç»œæœ€åä¸€å±‚æ˜¯ä¼ ç»Ÿçš„åˆ†ä½æ•°å›å½’ã€‚åˆ©ç”¨ first order feature importanceï¼Œé‡åŒ–è¾“å…¥å˜é‡çš„æ•´ä½“é‡è¦æ€§ã€‚åŒæ—¶æ’é™¤æ‰äºŒé˜¶çš„å’Œäº¤äº’çš„åœ¨åˆ†ä½æ•°ä¸­æ¥è¿‘äºé›¶ã€‚å› æ­¤ QRNN å’Œåˆ†ä½æ•° QR çš„åˆ†ä½æ•°æŸå¤±éå¸¸ç›¸ä¼¼
é€šè¿‡å…è®¸åˆ†ä½æ•°å›å½’ç¥ç»ç½‘ç»œå®ç°çš„åˆ†ä½æ•°ä¸­çš„éçº¿æ€§å’Œç›¸äº’ä½œç”¨æ¥æ‰©å±•è¿™ç§æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å¤§å¤§å¢å¼ºäº†å»ºæ¨¡çš„çµæ´»æ€§ã€‚é¢å¤–çš„çµæ´»æ€§åœ¨æ›´å¥½åœ°åˆ†å¸ƒæ‹Ÿåˆå’Œè¶…æ—¶æ ·æœ¬æ–¹é¢å¸¦æ¥äº†å›æŠ¥ï¼Œåˆ†ä½æ•°é¢„æµ‹ç²¾åº¦æé«˜äº† 30%ã€‚åŒæ—¶æ›´åŠ  robust ã€‚

å½“å‰æœºå™¨å­¦ä¹ æœ€ç«çƒ­çš„ä¸¤ä¸ªåº”ç”¨æ–¹å‘æ˜¯è®¡ç®—æœºè§†è§‰ CV å’Œè‡ªç„¶è¯­è¨€å¤„ç† NLP ï¼Œäº¦æœ‰ä¸€äº›æ–‡çŒ®åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†åˆ†ææ–‡æœ¬æ•°æ®åšç ”ç©¶ã€‚
* æœºå™¨å­¦ä¹ é¢„æµ‹ä¿¡ç”¨è¯„çº§
** æ•°æ®è¯´æ˜
æ•°æ®æ¥è‡ª [[https://www.kaggle.com/datasets/agewerc/corporate-credit-rating][kaggle]]
(ä¸‹è½½å¥½çš„åœ¨ [[/files/corporate_rating.csv][è¿™é‡Œ]])

A list of 2029 credit ratings issued by major agencies such as Standard and Poors to big US firms (traded on NYSE or Nasdaq) from 2010 to 2016.

There are 30 features for every company of which 25 are financial indicators. They can be divided in:

1. Liquidity Measurement Ratios: currentRatio, quickRatio, cashRatio, daysOfSalesOutstanding
2. Profitability Indicator Ratios: grossProfitMargin, operatingProfitMargin, pretaxProfitMargin, netProfitMargin, effectiveTaxRate, returnOnAssets, returnOnEquity, returnOnCapitalEmployed
3. Debt Ratios: debtRatio, debtEquityRatio
4. Operating Performance Ratios: assetTurnover, fixedAssetTurnover
5. Cash Flow Indicator Ratios: operatingCashFlowPerShare, freeCashFlowPerShare, cashPerShare, operatingCashFlowSalesRatio, freeCashFlowOperatingCashFlowRatio
   #+begin_src jupyter-python
import pandas as pd

# df = pd.read_csv("./corporate_rating.csv", encoding="utf-8")
df = pd.read_csv("/Users/dcy/Code/erm/corporate_rating.csv", encoding="utf-8")
df.info()
   #+end_src

   #+RESULTS:
   #+begin_example
   <class 'pandas.core.frame.DataFrame'>
   RangeIndex: 2029 entries, 0 to 2028
   Data columns (total 31 columns):
    #   Column                              Non-Null Count  Dtype
   ---  ------                              --------------  -----
    0   Rating                              2029 non-null   object
    1   Name                                2029 non-null   object
    2   Symbol                              2029 non-null   object
    3   Rating Agency Name                  2029 non-null   object
    4   Date                                2029 non-null   object
    5   Sector                              2029 non-null   object
    6   currentRatio                        2029 non-null   float64
    7   quickRatio                          2029 non-null   float64
    8   cashRatio                           2029 non-null   float64
    9   daysOfSalesOutstanding              2029 non-null   float64
    10  netProfitMargin                     2029 non-null   float64
    11  pretaxProfitMargin                  2029 non-null   float64
    12  grossProfitMargin                   2029 non-null   float64
    13  operatingProfitMargin               2029 non-null   float64
    14  returnOnAssets                      2029 non-null   float64
    15  returnOnCapitalEmployed             2029 non-null   float64
    16  returnOnEquity                      2029 non-null   float64
    17  assetTurnover                       2029 non-null   float64
    18  fixedAssetTurnover                  2029 non-null   float64
    19  debtEquityRatio                     2029 non-null   float64
    20  debtRatio                           2029 non-null   float64
    21  effectiveTaxRate                    2029 non-null   float64
    22  freeCashFlowOperatingCashFlowRatio  2029 non-null   float64
    23  freeCashFlowPerShare                2029 non-null   float64
    24  cashPerShare                        2029 non-null   float64
    25  companyEquityMultiplier             2029 non-null   float64
    26  ebitPerRevenue                      2029 non-null   float64
    27  enterpriseValueMultiple             2029 non-null   float64
    28  operatingCashFlowPerShare           2029 non-null   float64
    29  operatingCashFlowSalesRatio         2029 non-null   float64
    30  payablesTurnover                    2029 non-null   float64
   dtypes: float64(25), object(6)
   memory usage: 491.5+ KB
   #+end_example

è¯„çº§åˆ†å¸ƒå¦‚ä¸‹å›¾ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯„çº§ CC, C, D çš„ä¼ä¸šæ•°é‡è¾ƒå°‘ã€‚ä¸‰å¤§è¯„çº§å…¬å¸æ‰€è°“çš„â€œDâ€æ˜¯è¿çº¦â€œDefaultâ€ï¼Œå› æ­¤æˆ‘ä»¬ä¿ç•™ä¸‹æ¥ D çº§ï¼Œè€Œåˆå¹¶ =CCC= =CC= =C= ã€‚ä¸€æ–¹é¢æ˜¯ç”±äº CCC ä»¥ä¸‹æ•°é‡å°‘ï¼Œå¦ä¸€æ–¹é¢æ˜¯ç”±äºå¤§å¤šæ•°â€œè¯„çº§ä¸‹è°ƒåŠ é€Ÿåˆ°æœŸâ€æ¡æ¬¾é™å®šåœ¨äº†é™è‡³ CCC çš„åƒåœ¾çº§ã€‚ç±»ä¼¼çš„ï¼Œç”±äº AAA ä¼ä¸šæ•°é‡å¾ˆå°‘éƒ½æ˜¯éå¸¸ä¼˜è´¨çš„ä¼ä¸šï¼ˆ +ä¸åƒç›®å‰å›½å†…è¯„çº§æ–°å‘å€ºä¸€åŠä¸º AAA+ ï¼‰ï¼Œè€Œ AA å’Œ A æ•°é‡éƒ½ä¸å°ï¼Œæˆ‘ä»¬ä»ç„¶å•ç‹¬æŠŠä»–ä»¬æ‹¿å‡ºæ¥ã€‚
#+begin_src jupyter-python
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(
    style="white",
    context="paper",
    rc={"text.usetex": True},
)
df["Rating"].value_counts().plot(kind="bar")
#+end_src

#+RESULTS:
:RESULTS:
: <AxesSubplot:>
[[file:./.ob-jupyter/2475fbd207d00348d4aff2b5db9de9f766ebc7a8.png]]
:END:

è®©æˆ‘ä»¬å¤„ç†ä¸€ä¸‹æ•°æ®
#+begin_src jupyter-python
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, recall_score, precision_score
RANDOM_STATE = 42
Y = df["Rating"]
Y = Y.replace({"CCC": "C", "CC": "C"})
df["Date"] = df["Date"].apply(lambda x: x.split("/")[-1])
dummies = ["Rating Agency Name", "Sector", "Date"]
X = df[[i for i in df.columns if df[i].dtype != "object"]]
for dummy in dummies:
    X = pd.concat([X, pd.get_dummies(df[dummy], drop_first=True, prefix=dummy)], axis=1)
Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.25, random_state=RANDOM_STATE)
result = {}
X.columns
#+end_src

#+RESULTS:
#+begin_example
Index(['currentRatio', 'quickRatio', 'cashRatio', 'daysOfSalesOutstanding',
       'netProfitMargin', 'pretaxProfitMargin', 'grossProfitMargin',
       'operatingProfitMargin', 'returnOnAssets', 'returnOnCapitalEmployed',
       'returnOnEquity', 'assetTurnover', 'fixedAssetTurnover',
       'debtEquityRatio', 'debtRatio', 'effectiveTaxRate',
       'freeCashFlowOperatingCashFlowRatio', 'freeCashFlowPerShare',
       'cashPerShare', 'companyEquityMultiplier', 'ebitPerRevenue',
       'enterpriseValueMultiple', 'operatingCashFlowPerShare',
       'operatingCashFlowSalesRatio', 'payablesTurnover',
       'Rating Agency Name_Egan-Jones Ratings Company',
       'Rating Agency Name_Fitch Ratings',
       'Rating Agency Name_Moody's Investors Service',
       'Rating Agency Name_Standard & Poor's Ratings Services',
       'Sector_Capital Goods', 'Sector_Consumer Durables',
       'Sector_Consumer Non-Durables', 'Sector_Consumer Services',
       'Sector_Energy', 'Sector_Finance', 'Sector_Health Care',
       'Sector_Miscellaneous', 'Sector_Public Utilities', 'Sector_Technology',
       'Sector_Transportation', 'Date_2009', 'Date_2010', 'Date_2011',
       'Date_2012', 'Date_2013', 'Date_2014', 'Date_2015', 'Date_2016'],
      dtype='object')
#+end_example

=get_score= ä¸­å®šä¹‰äº†ä¸‰é‡ç»´åº¦æ¥åº¦é‡é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¦‚ä¸‹è¡¨ã€‚precision æ˜¯ \(tp / (tp + fp)\) ï¼Œå³é¢„æµ‹é˜³æ€§ä¸­çœŸå®ä¸ºæ­£çš„æ¦‚ç‡ï¼›recall æ˜¯ \(tp / (tp + fn)\) ï¼Œå³æ ·æœ¬ä¸­çš„æ­£ä¾‹æœ‰å¤šå°‘è¢«é¢„æµ‹æ­£ç¡®äº†ï¼›è€Œ f1 åˆ™æ˜¯äºŒè€…çš„è°ƒå’Œå¹³å‡

|          | True | False |
| Positive | TP   | FP    |
| Negative | TN   | FN    |

é‰´äºè¯„çº§æœ‰ä¸ƒä¸ªï¼Œå®Œå®Œå…¨å…¨çš„å‡†ç¡®ç‡å¯èƒ½æ²¡æœ‰é‚£ä¹ˆé«˜ï¼Œæˆ‘ä»¬åšä¸€ä¸ªéšæœºçš„æµ‹è¯•ï¼Œä½œä¸ºåŸºå‡†ã€‚

#+begin_src jupyter-python :kernel python3 :session main
from scipy.stats import pearsonr


def get_score(Xtest, Ytrue, model):
    Ypred = model(Xtest)
    average = "weighted"
    rating_map = {i: ord(i[0]) * 100 - len(i) for i in Y.unique()}
    return {
        "precision": precision_score(Ytrue, Ypred, average=average, zero_division=0),
        "recall": recall_score(Ytrue, Ypred, average=average),
        "f1": f1_score(Ytrue, Ypred, average=average),
        "\(R^2\)": pearsonr(
            [rating_map[i] for i in Ypred], [rating_map[i] for i in Ytest]
        )[0],
    }


import random
import numpy as np
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)
ratings = Y.unique()
tmp = {}
monte_num = 100
for i in range(100):
    Ypredict = Xtest.index.map(lambda x: random.choice(ratings))
    monte = get_score(Xtest, Ytest, lambda _: Ypredict)
    for j in monte:
        if j not in tmp:
            tmp[j] = 0
        tmp[j] += monte[j]
result["random"] = {i: tmp[i] / 100 for i in tmp}
result["random"]
#+end_src

#+RESULTS:
| precision | : | 0.23640721599080028 | recall | : | 0.12547244094488194 | f1 | : | 0.1544436285781241 | \(R^2\) | : | 0.008907176874420717 |

** çº¿æ€§å›å½’ä¸å†³ç­–æ ‘
æˆ‘ä»¬å…ˆçœ‹ä¸€äº›ç®€å•ç›´æ¥çš„ä¾‹å­ã€‚

æŒ‰ç…§ç»´åŸºç™¾ç§‘çš„å®šä¹‰ï¼Œæˆ‘ä»¬åœ¨è®¡é‡ç»æµå­¦ä¸­å­¦ä¹ çš„ OLS/GLS/Logit æ¨¡å‹ä¹Ÿæ˜¯é€šè¿‡æœºå™¨æ¥å­¦ä¹ æ‹Ÿåˆæ ·æœ¬çš„åˆ†å¸ƒï¼Œä¹Ÿæ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ã€‚ç»Ÿè®¡å­¦ä¸­çš„ lasso/ridge ç­‰å›å½’æ–¹å¼ä¹Ÿåœ¨æ¨¡å‹æ³›åŒ–ä¸­æœ‰è®¸å¤šåº”ç”¨ã€‚
#+begin_src jupyter-python :kernel python3 :session main
from sklearn.linear_model import LogisticRegression

logit = LogisticRegression(multi_class="multinomial", solver="saga", random_state=RANDOM_STATE)
logit.fit(Xtrain, Ytrain)
result["logit"] = get_score(Xtest, Ytest, logit.predict)
result["logit"]
#+end_src

#+RESULTS:
:RESULTS:
: /Users/dcy/Code/erm/.venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
:   warnings.warn(
| precision | : | 0.18152641834788635 | recall | : | 0.2440944881889764 | f1 | : | 0.15470492292394605 | \(R^2\) | : | -0.01775335686060572 |
:END:

å†³ç­–æ ‘ä¹Ÿåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æœ‰åº”ç”¨ï¼Œè½¦é™©å®šä»·æˆ–è€…æˆ‘ä»¬æ—¥å¸¸çš„å†³ç­–éƒ½å¯ä»¥æŠ½è±¡æˆå†³ç­–æ ‘ã€‚
ä»–çš„æ€æƒ³æ˜¯ï¼Œä¸€ä¸ªæ•°æ®é›†æœ‰å¤šä¸ªç‰¹å¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹æŒ‰ç…§æŸä¸ªç‰¹å¾æ˜¯å¦æ»¡è¶³ä¸€å®šçš„æ¡ä»¶åˆ†å‰ï¼Œå½¢æˆä¸€æ£µäºŒå‰æ ‘ã€‚
è¯¥èŠ‚ç‚¹é€‰å–ç‰¹å¾åˆ†å‰çš„å†³ç­–ä¾æ®æ˜¯æœ€å¤§åŒ–â€œä¿¡æ¯å¢ç›Šâ€ï¼Œå³åˆ†å‰å‰åæ•°æ®æ›´â€œæœ‰åºâ€ï¼Œä¸”æ›´æœ‰åºçš„ç¨‹åº¦æœ€å¤§ï¼Œå¸¸è§æŒ‡æ ‡çš„æœ‰ä¿¡æ¯ç†µ/åŸºå°¼ç³»æ•°ç­‰ã€‚
è¿™æ£µæ ‘ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬ä¼šå¯¹å†³ç­–æ ‘â€œå‰ªæâ€ï¼Œå¢åŠ ä¸€äº›åˆ†æ”¯æ¡ä»¶çš„é™åˆ¶ï¼Œå¯ä»¥çœ‹[[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html][è¿™é‡Œ]]ã€‚

å†³ç­–æ ‘å¥½å¤„æ˜¯è®¡ç®—é‡ç®€å•ï¼Œå¯è§£é‡Šæ€§å¼ºï¼Œæ¯”è¾ƒé€‚åˆå¤„ç†æœ‰ç¼ºå¤±å±æ€§å€¼çš„æ ·æœ¬ï¼Œèƒ½å¤Ÿå¤„ç†ä¸ç›¸å…³çš„ç‰¹å¾ï¼›ä½†æ˜¯å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
#+begin_src jupyter-python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)
dt.fit(Xtrain, Ytrain)
result["decision tree"] = get_score(Xtest, Ytest, dt.predict)
result["decision tree"]
#+end_src

#+RESULTS:
| precision | : | 0.3498883972871522 | recall | : | 0.3799212598425197 | f1 | : | 0.3528756642882673 | \(R^2\) | : | 0.3632276367156315 |

** é›†æˆå­¦ä¹ 
ensemble learning æ˜¯å•ä¸ªæ¨¡å‹å¹¶ä¸èƒ½å¾ˆå®Œç¾çš„è§£å†³æŸä¸ªåˆ†ç±»æˆ–è€…å›å½’é—®é¢˜ï¼ˆå¼±ç›‘ç£æ¨¡å‹ï¼Œåœ¨æŸäº›æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼‰çš„æ—¶å€™ï¼Œé‚£ä¹ˆå°±è®­ç»ƒå‡ºå¤šä¸ªå¼±ç›‘ç£æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹å¯èƒ½æ˜¯ç›¸åŒçš„ä¹Ÿå¯ä»¥æ˜¯ä¸åŒçš„ï¼Œç„¶åé¢„æµ‹çš„æ—¶å€™å°†æ•°æ®åˆ†åˆ«è¾“å…¥æ¯ä¸ªæ¨¡å‹ï¼Œæœ€åå°†æ¯ä¸ªæ¨¡å‹çš„è¾“å‡ºç»¼åˆèµ·æ¥ä½œä¸ºè¯¥æœªçŸ¥æ•°æ®çš„è¾“å‡ºå³ä¾¿æŸä¸€ä¸ªå¼±åˆ†ç±»å™¨å¾—åˆ°äº†é”™è¯¯çš„é¢„æµ‹ï¼Œå…¶ä»–çš„å¼±åˆ†ç±»å™¨ä¹Ÿå¯ä»¥å°†é”™è¯¯çº æ­£å›æ¥ã€‚ç®€è€Œè¨€ä¹‹ï¼Œé‡‡æ ·-å­¦ä¹ -ç»„åˆã€‚

å¦‚ä½•è®­ç»ƒå’Œè¾“å‡ºå‘¢ï¼Ÿ
*** bagging
Baggingæ˜¯bootstrap aggregatingçš„ç®€å†™ã€‚åœ¨ bagging æ–¹æ³•ä¸­ï¼Œä»æ•´ä½“æ•°æ®é›†ä¸­é‡‡å–æœ‰æ”¾å›æŠ½æ ·å¾—åˆ°Nä¸ªæ•°æ®é›†ï¼Œåœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šå­¦ä¹ å‡ºä¸€ä¸ªæ¨¡å‹ã€‚

éšæœºæ£®æ—å°±æ˜¯é‡‡ç”¨äº† bagging çš„æ–¹å¼è®­ç»ƒäº†è®¸å¤šæ£µå†³ç­–æ ‘ï¼Œæ˜¯ä¸ºâ€œæ£®æ—â€ã€‚åœ¨è¾“å‡ºæ—¶ï¼Œæ¯ä¸€æ£µæ ‘éƒ½å°†å…¶ç»“æœâ€œæŠ•ç¥¨â€ï¼Œå“ªä¸ªç±»åˆ«å¤šï¼Œè¾“å…¥æ ·æœ¬å°±å±äºå“ªä¸ªç±»åˆ«ã€‚

#+begin_src jupyter-python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=RANDOM_STATE)
rf.fit(Xtrain, Ytrain)
result["random forest"] = get_score(Xtest, Ytest, rf.predict)
result["random forest"]
#+end_src

#+RESULTS:
| precision | : | 0.39603489727136376 | recall | : | 0.4251968503937008 | f1 | : | 0.38351839772007446 | \(R^2\) | : | 0.39959139741930844 |

Baggingä¸»è¦å…³æ³¨é™ä½æ–¹å·®ï¼Œå› æ­¤å®ƒåœ¨ä¸å‰ªæçš„å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰å­¦ä¹ å™¨ä¸Šæ•ˆç”¨æ›´ä¸ºæ˜æ˜¾ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

#+CAPTION: random forest
[[https://tfugcs.andfun.cn/original/2X/7/74f5a02b7692010da60a746d5469471c68b2ff3c.gif]]
*** boosting
bagging çš„è®­ç»ƒæ˜¯å¹³è¡Œçš„ï¼Œboosting åˆ™æ˜¯è¿­ä»£åœ°è®­ç»ƒä¸€ç³»åˆ—çš„åˆ†ç±»å™¨ï¼Œæ¯ä¸ªåˆ†ç±»å™¨é‡‡ç”¨çš„æ ·æœ¬åˆ†å¸ƒéƒ½å’Œä¸Šä¸€è½®çš„å­¦ä¹ ç»“æœæœ‰å…³ï¼Œç›´è§‚æ¯”æ–¹æ˜¯æ¯ä¸ªæ ‘éƒ½å»å­¦ä¹ ä¸Šä¸€ä¸ªæ ‘æ²¡æœ‰å­¦ä¹ å¥½çš„åœ°æ–¹ï¼Œä»£è¡¨ç®—æ³•æœ‰AdaBoostï¼ˆAdaptive boostingï¼‰ç®—æ³•ï¼Œä»¥åŠ XGBoost ç®—æ³•ã€‚
è°ƒå‚æ—¶å¯ä»¥æ ‘çš„æ·±åº¦å¾ˆå°‘å°±èƒ½è¾¾åˆ°å¾ˆé«˜çš„ç²¾åº¦ã€‚
[[https://www.researchgate.net/publication/351542039/figure/fig1/AS:1022852723662850@1620878501807/Flow-diagram-of-gradient-boosting-machine-learning-method-The-ensemble-classifiers.png]]

#+begin_src jupyter-python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=RANDOM_STATE)

gb.fit(Xtrain, Ytrain)
result["gradient boosting"] = get_score(Xtest, Ytest, gb.predict)
result["gradient boosting"]
#+end_src

#+RESULTS:
| precision | : | 0.530520009176101 | recall | : | 0.5255905511811023 | f1 | : | 0.5094674767985568 | \(R^2\) | : | 0.5421094375792002 |

** æ”¯æŒå‘é‡æœº
Support Vector Machine, SVM æ˜¯ä¸€ç§äºŒåˆ†ç±»å™¨ï¼Œå…¶æ€æƒ³æ˜¯æ ·æœ¬åˆ†å¸ƒåœ¨ç©ºé—´ä¸­ï¼Œæ‰¾åˆ°ä¸€ä¸ªå¯ä»¥åˆ’åˆ†å¼€æ ·æœ¬ç‚¹ã€å¹¶ä¸”é—´éš”æœ€å¤§çš„çš„ï¼ˆè¶…ï¼‰å¹³é¢ã€‚ç›´è§‚ä¸Šé—´éš”æœ€å¤§æ˜¯ä¸ºäº†è®©æ¨¡å‹æ›´ç¨³å¥ã€‚

#+CAPTION: SVM å›¾ç¤º
#+NAME: SVM å›¾ç¤º
[[https://pic2.zhimg.com/80/v2-f9e1e7fd08460a5fab044c71ed8b0bb1_1440w.jpg]]

æœ€ç®€å•çš„çº¿æ€§çš„ç¡¬é—´éš”å¯åˆ†çš„å¦‚å›¾ [[SVM å›¾ç¤º]] æ‰€ç¤ºï¼Œå½“ç„¶è¿™æ˜¯æ¯”è¾ƒç†æƒ³çš„æƒ…å†µã€‚å½“æ ·æœ¬åˆ†å¸ƒæ›´å¤æ‚çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©è½¯é—´éš”ï¼Œå³å°†ä¹‹å‰çš„ç¡¬é—´éš”æœ€å¤§åŒ–æ¡ä»¶æ”¾å®½ä¸€ç‚¹ï¼Œå…è®¸éƒ¨åˆ†ç‚¹å‡ºé”™ï¼Œåœ¨ä¼˜åŒ–å‡½æ•°ä¸­åŠ å…¥æƒ©ç½šé¡¹ã€‚

å¦‚æœè¿˜æ˜¯ä¸å¯ä»¥ï¼Œæˆ‘ä»¬ä¼šè¿ç”¨æ ¸å‡½æ•°æ¥æ¨å¯¼åˆ°éçº¿å½¢çš„æƒ…å†µï¼Œç®€å•è¯´å°±æ˜¯å°†ä½ç»´çš„æ ·æœ¬ç‚¹æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œä½¿æ ·æœ¬çº¿æ€§å¯åˆ†ã€‚ä¾‹å¦‚å†…ç§¯å¹³æ–¹çš„æ ¸å‡½æ•°ï¼Œ\(K(v_1,V_2)=(x_1x_2+y_1y_2)^2\)ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä¸‰ç»´ç©ºé—´ä¸­ \((x_i^2,\sqrt{2}x_iy_i,y_i^2)\) ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»


#+begin_src jupyter-python
from sklearn.svm import SVC

svm = SVC(kernel="rbf", gamma="auto", random_state=RANDOM_STATE)
svm.fit(Xtrain, Ytrain)
result["SVM"] = get_score(Xtest, Ytest, svm.predict)
result["SVM"]
#+end_src

#+RESULTS:
| precision | : | 0.4136927083234441 | recall | : | 0.4094488188976378 | f1 | : | 0.351708147106921 | \(R^2\) | : | 0.3431290118925812 |

** KNN
è¿™é‡Œçš„ NN ä¸æ˜¯åæ–‡çš„ CNN ç­‰çš„ç¥ç»ç½‘ç»œï¼Œå…¨ç§°æ˜¯K Nearest Neighborsï¼Œæ„æ€æ˜¯æŸä¸ªç‚¹åˆ†ç±»å–å†³äº K ä¸ªæœ€è¿‘çš„é‚»å±…

#+begin_src jupyter-python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

KNN = KNeighborsClassifier(n_neighbors=3)
KNN.fit(Xtrain, Ytrain)
result["KNN"] = get_score(Xtest, Ytest, KNN.predict)
result["KNN"]
#+end_src

#+RESULTS:
| precision | : | 0.3625400456087721 | recall | : | 0.35236220472440943 | f1 | : | 0.34202311550427716 | \(R^2\) | : | 0.29873782865224163 |

** K means
#+begin_quote
æœ‰å››ä¸ªç‰§å¸ˆå»éƒŠåŒºå¸ƒé“ï¼Œä¸€å¼€å§‹ç‰§å¸ˆä»¬éšæ„é€‰äº†å‡ ä¸ªå¸ƒé“ç‚¹ï¼Œå¹¶ä¸”æŠŠè¿™å‡ ä¸ªå¸ƒé“ç‚¹çš„æƒ…å†µå…¬å‘Šç»™äº†éƒŠåŒºæ‰€æœ‰çš„æ‘æ°‘ï¼Œäºæ˜¯æ¯ä¸ªæ‘æ°‘åˆ°ç¦»è‡ªå·±å®¶æœ€è¿‘çš„å¸ƒé“ç‚¹å»å¬è¯¾ã€‚

å¬è¯¾ä¹‹åï¼Œå¤§å®¶è§‰å¾—è·ç¦»å¤ªè¿œäº†ï¼Œäºæ˜¯æ¯ä¸ªç‰§å¸ˆç»Ÿè®¡äº†ä¸€ä¸‹è‡ªå·±çš„è¯¾ä¸Šæ‰€æœ‰çš„æ‘æ°‘çš„åœ°å€ï¼Œæ¬åˆ°äº†æ‰€æœ‰åœ°å€çš„ä¸­å¿ƒåœ°å¸¦ï¼Œå¹¶ä¸”åœ¨æµ·æŠ¥ä¸Šæ›´æ–°äº†è‡ªå·±çš„å¸ƒé“ç‚¹çš„ä½ç½®ã€‚

ç‰§å¸ˆæ¯ä¸€æ¬¡ç§»åŠ¨ä¸å¯èƒ½ç¦»æ‰€æœ‰äººéƒ½æ›´è¿‘ï¼Œæœ‰çš„äººå‘ç°Aç‰§å¸ˆç§»åŠ¨ä»¥åè‡ªå·±è¿˜ä¸å¦‚å»Bç‰§å¸ˆå¤„å¬è¯¾æ›´è¿‘ï¼Œäºæ˜¯æ¯ä¸ªæ‘æ°‘åˆå»äº†ç¦»è‡ªå·±æœ€è¿‘çš„å¸ƒé“ç‚¹â€¦â€¦

å°±è¿™æ ·ï¼Œç‰§å¸ˆæ¯ä¸ªç¤¼æ‹œæ›´æ–°è‡ªå·±çš„ä½ç½®ï¼Œæ‘æ°‘æ ¹æ®è‡ªå·±çš„æƒ…å†µé€‰æ‹©å¸ƒé“ç‚¹ï¼Œæœ€ç»ˆç¨³å®šäº†ä¸‹æ¥ã€‚
#+end_quote

ä¹‹å‰æåˆ°çš„ç®—æ³•éƒ½éœ€è¦å¯¹æ•°æ®è¿›è¡Œä¸€å®šçš„æ ‡æ³¨ï¼Œæ ‡å¥½æŸäº›æ•°æ®å±äºæŸä¸ªåˆ†ç±»ï¼Œä¹Ÿå°±æ˜¯å¸¸è¯´çš„â€œç›‘ç£å­¦ä¹ â€ã€‚K-means æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ ï¼Œæˆ‘ä»¬ä¸éœ€è¦å£°æ˜è®­ç»ƒä¸­çš„å“ªäº›æ•°æ®æ˜¯å“ªä¸ªåˆ†ç±»ã€‚

K-means çš„æ–¹æ³•æ˜¯ï¼Œé€‰æ‹©åˆå§‹åŒ–çš„ k ä¸ªæ ·æœ¬ä½œä¸ºåˆå§‹èšç±»ä¸­å¿ƒ \(a_i\)  ï¼Œé’ˆå¯¹æ•°æ®é›†ä¸­æ¯ä¸ªæ ·æœ¬ \(x_i\)
è®¡ç®—å®ƒåˆ° k ä¸ªèšç±»ä¸­å¿ƒçš„è·ç¦»ï¼Œå¹¶å°†å…¶åˆ†åˆ°è·ç¦»æœ€å°çš„èšç±»ä¸­å¿ƒæ‰€å¯¹åº”çš„ç±»ä¸­ï¼›é‡æ–°è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è´¨å¿ƒä½œä¸ºèšç±»ä¸­å¿ƒ \(a_i\) ï¼Œå†é‡å¤ä¸Šé¢çš„è¿‡ç¨‹ï¼Œç›´è‡³èšç±»ä¸­å¿ƒâ€œç¨³å®šâ€ä¸‹æ¥ã€‚
#+begin_src python :result output
from sklearn.cluster import KMeans
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0]])
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
kmeans.predict([[0, 0], [12, 3]])
#+end_src

** æ·±åº¦å­¦ä¹ /ç¥ç»ç½‘ç»œ
æ·±åº¦å­¦ä¹ ä»¥ç¥ç»ç½‘ç»œä¸ºåŸºç¡€ã€‚ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿ç”Ÿç‰©ç¥ç»ç³»ç»Ÿç»“æ„å’ŒåŠŸèƒ½çš„æ•°å­¦æ¨¡å‹ï¼Œå¯¹å‡½æ•°è¿›è¡Œä¼°è®¡å’Œè¿‘ä¼¼ã€‚
*** å¤šå±‚æ„ŸçŸ¥æœº
æ˜¯æ·±åº¦å­¦ä¹ çš„å…¥é—¨ç®—æ³•ï¼Œè¯¯å·®åå‘ä¼ æ’­ Backpropagationï¼Œåˆºæ¿€æ­£å‘ä¼ æ’­åé€šè¿‡æ¢¯åº¦ä¸‹é™çš„æ–¹å¼æœ€å°åŒ–è¯¯å·®åå‘ä¼ æ’­æ›´æ–°æƒå€¼ï¼ˆæœ€å°åŒ–çš„æ–¹å¼æ˜¯â€œæ¢¯åº¦ä¸‹é™â€ï¼‰ã€‚å®ƒçš„ä¿¡æ¯å¤„ç†èƒ½åŠ›æ¥æºäºç®€å•éçº¿æ€§å‡½æ•°çš„å¤šæ¬¡å¤åˆã€‚

**** æ¢¯åº¦ä¸‹é™ä¸åå‘ä¼ æ’­

æˆ‘ä»¬ç”¨æœ€å°äºŒä¹˜æ³•æ¥ç†è§£â€œæ¢¯åº¦ä¸‹é™â€å’Œâ€œåå‘ä¼ æ’­â€
#+begin_src jupyter-python :session reg
import torch
x = torch.rand([500,1]) # X æ˜¯ä¸€ä¸ª tensor ï¼Œå¯ä»¥æŠŠä»–æƒ³è±¡æˆ 500x1 çš„å‘é‡
y_true = 3*x+8
learning_rate = 0.05 # learning rate æ˜¯æ¯æ¬¡æ¢¯åº¦ä¸‹é™çš„â€œæ­¥é•¿â€
w = torch.rand([1,1], requires_grad=True) # w å’Œ b æˆ‘ä»¬è¦ pytorch è‡ªåŠ¨æ±‚å¯¼
b = torch.tensor(0, requires_grad=True, dtype=torch.float32)
for i in range(500):
    y_pred = torch.matmul(x,w)+b # é¢„æµ‹æ˜¯å¤šå°‘
    loss = (y_true-y_pred).pow(2).mean() # æŸå¤±
    if w.grad is not None: # æŠŠä¸Šä¸€æ¬¡çš„æ¢¯åº¦æ¸…é›¶
        w.grad.data.zero_()
    if b.grad is not None:
        b.grad.data.zero_()
    loss.backward() # è¯¯å·®åå‘ä¼ æ’­ï¼Œå¾—åˆ° w å’Œ b çš„æ¢¯åº¦
    w.data = w.data - w.grad*learning_rate # æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æ–°çš„ w å’Œ b
    b.data = b.data - b.grad*learning_rate
    if i % 50 == 0:
        print(w.item(), b.item(), loss.item())
#+end_src

#+RESULTS:
: 1.386063575744629 0.9126168489456177 83.67776489257812
: 4.305609703063965 7.24848747253418 0.1539001762866974
: 3.9357428550720215 7.46671199798584 0.07863160222768784
: 3.6690192222595215 7.618724822998047 0.04019376263022423
: 3.4783215522766113 7.72740364074707 0.0205457154661417
: 3.34197998046875 7.805105209350586 0.010502253659069538
: 3.244502067565918 7.860658168792725 0.005368418991565704
: 3.174808979034424 7.900376319885254 0.002744155703112483
: 3.124981641769409 7.928772449493408 0.001402730355039239
: 3.0893561840057373 7.949076175689697 0.000717016460839659

ä¸Šè¿°çš„ä»£ç åœ¨ pytorch ä¸­å¯¹åº”çš„æœ‰ï¼š
| =for= å¾ªç¯é‡Œé¢çš„æ¨¡å‹ | =nn.Module= å°è£…å¥½äº†è®¸å¤šæ¨¡å‹         |
| =loss= çš„å®šä¹‰        | torch ä¸­ä¹Ÿæœ‰å¤šç§è®¡ç®—æ–¹å¼           |
| =loss= çš„è®¡ç®—        | ä¼˜åŒ–å™¨ =nn.optim= ä¸­æä¾›äº†è®¸å¤šä¼˜åŒ–å™¨ |
é€šè¿‡ pytorch æˆ‘ä»¬å¯ä»¥å†™æˆ
#+begin_src python
import torch
from torch import nn
from torch import optim

x = torch.rand([50,1])
y = 3*x+8

class Lr(nn.Module):
    def __init__(self):
        super(Lr, self).__init__()
        self.layer = nn.Linear(1,1)
    def forward(self, x):
        return self.layer(x)
model = Lr()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.05)
for i in range(500):
    out = model(x)
    loss = criterion(y, out)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
list(model.parameters())
#+end_src

**** æ¿€æ´»å‡½æ•°

[[https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Action_potential.svg/718px-Action_potential.svg.png]]

ç¥ç»ç½‘ç»œæœ¬æ„æ˜¯æƒ³æ¨¡ä»¿ç¥ç»å…ƒã€‚é«˜ä¸­æˆ‘ä»¬å­¦è¿‡ç¥ç»å—åˆ°åˆºæ¿€åä¸ä¸€å®šä¼šäº§ç”Ÿç”µä¿¡å·ï¼Œè€Œæ˜¯éœ€è¦è¾¾åˆ°é˜ˆå€¼åæ‰èƒ½äº§ç”ŸåŠ¨ä½œç”µä½ã€‚å› æ­¤å½“ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚æ”¶åˆ°ä¿¡å·ä¼ å¯¼ç»™éšè—å±‚åï¼Œéšè—å±‚æ˜¯ç›´æ¥å‘è¾“å‡ºå±‚ä¼ å¯¼ï¼ˆè¿™æ ·çš„è¯é€šè¿‡ç¥ç»ç½‘ç»œçº¿æ€§å‡½æ•°çš„å åŠ ä»ç„¶æ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼‰ï¼Œè€Œæ˜¯è¦ç»å†ä¸€ä¸ªéçº¿æ€§çš„â€œæ¿€æ´»å‡½æ•°â€ï¼Œå¦‚ =relu= , =sigmoid=, =softsign= ï¼Œç„¶åå†è¿›è¡Œä¼ å¯¼ã€‚å³é’ˆå¯¹ \(X\) è¾“å…¥ï¼Œç¥ç»å…ƒè¾“å‡ºä¼šæ˜¯ \(f(W^TX+b)\) ã€‚

æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå¯è§†åŒ–åœ°ç†è§£ä¸€ä¸‹
https://playground.tensorflow.org/
**** ä¸€ä¸ªå°è¯•

è¿™æ˜¯æˆ‘ç”¨ä¸¤å±‚ç¥ç»ç½‘ç»œçš„ä»£ç 
#+begin_src jupyter-python
from torch import nn
import torch
torch.manual_seed(42)
Ytrain_nn = pd.get_dummies(Ytrain)
encode = Ytrain_nn.columns
Ytrain_nn = torch.tensor(Ytrain_nn.values, dtype=torch.float32)
Xtrain_nn = torch.tensor(Xtrain.values, dtype=torch.float32)

hidden_layer = 40
net = nn.Sequential(
    nn.Linear(Xtrain_nn.shape[1], hidden_layer),
    nn.ReLU(),
    nn.Linear(hidden_layer, len(encode)),
    nn.Softmax(dim=1),
)
optimizer = torch.optim.SGD(net.parameters(), lr=0.001)
loss_func = torch.nn.MSELoss()

for t in range(10000):
    prediction = net(Xtrain_nn)
    loss = loss_func(Ytrain_nn, prediction)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
Xtest_nn = torch.tensor(Xtest.values, dtype=torch.float32)
prediction = pd.DataFrame(net(Xtest_nn).detach().numpy())
Ypredict = prediction.idxmax(axis=1).map(lambda x: encode[x])
result["bp neural network"] = get_score(Xtest, Ytest, lambda _: Ypredict)
result["bp neural network"]
#+end_src

#+RESULTS:
| precision | : | 0.27389121797460775 | recall | : | 0.33858267716535434 | f1 | : | 0.27631891749192855 | \(R^2\) | : | -0.0591120710799986 |

*** CNN
æ‰€è°“å·ç§¯ç¥ç»ç½‘ç»œï¼Œå°±æ˜¯ç”¨å·ç§¯æ ¸æ‰«æï¼Œç±»ä¼¼â€œé”åŒ–â€ï¼Œæ˜¯ä¸€ç§æ¯”è¾ƒç»å…¸çš„è®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚å›¾ç‰‡ä¹‹é—´çš„åƒç´ æ˜¯æœ‰å…³ç³»çš„ï¼Œåˆšåˆšçš„ç¥ç»ç½‘ç»œæ˜¾ç„¶æ²¡æœ‰è€ƒè™‘åˆ°è¿ç»­åƒç´ çš„å…³è”æ€§ï¼ŒCNN é€šè¿‡åšå·ç§¯å°†å…³ç³»å‘ˆç°å‡ºæ¥ã€‚
[[https://pic2.zhimg.com/v2-ede517995e1604d6f96cc01614d320b9_b.jpg]]

[[https://zh.m.wikipedia.org/zh-hans/%E5%8D%B7%E7%A7%AF][å·ç§¯]]æœ‰å…¶æ•°å­¦å®šä¹‰ \((f*g)(n) = \int_{-\infty}^{\infty}f(\tau)g(n-\tau)\mathrm{d}\tau\)ï¼Œç®€å•åœ°ç†è§£å°±æ˜¯ä¸¤ä¸ªå‡½æ•° =f= å’Œ =g= ï¼Œå…ˆå¯¹gå‡½æ•°è¿›è¡Œç¿»è½¬ï¼Œç›¸å½“äºåœ¨æ•°è½´ä¸ŠæŠŠ =g= å‡½æ•°ä»å³è¾¹â€œå·â€åˆ°å·¦è¾¹å»ã€‚ç„¶åå†æŠŠ =g= å‡½æ•°å¹³ç§»åˆ° =n= ï¼Œåœ¨è¿™ä¸ªä½ç½®å¯¹ä¸¤ä¸ªå‡½æ•°çš„å¯¹åº”ç‚¹ç›¸ä¹˜ï¼Œç„¶åç›¸åŠ ï¼ˆâ€œç§¯â€ï¼‰ã€‚

å·ç§¯ç¥ç»ç½‘ç»œå…ˆç”¨å·ç§¯å±‚æ‰«æå‡ºç‰¹å¾ï¼Œç„¶ååˆ©ç”¨â€œæ± åŒ–â€å¢å¼ºç¨³å¥æ€§é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæœ€åä¸€ä¸ªå…¨è¿æ¥å±‚å¤„ç†è¾“å‡ºã€‚å›¾åƒå¯ä»¥ç”±äºŒç»´çš„ä½ç½®å’Œç¬¬ä¸‰ç»´ï¼ˆé¢œè‰² RGB ï¼‰ç¡®å®šï¼Œåœ¨ =pytorch= ä¸­å¸¸ç”¨ =Conv2d= ã€‚è€Œæˆ‘ä»¬çš„æ•°æ®åˆ™æ˜¯ä¸€æ¡æ¡çš„ï¼Œæœ›æ–‡ç”Ÿä¹‰åº”è¯¥ç”¨ =Conv1d= ï¼ˆå…¶å®ä¼šç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œä½† RNN åº”ç”¨æ›´å¤šï¼‰ã€‚

ä»è¿™é‡Œå¼€å§‹åˆ©ç”¨ CPU è®­ç»ƒæ¯”è¾ƒæ…¢ï¼Œæœ‰ NVIDIA GPU çš„åŒå­¦å¯ä»¥å°è¯•åœ¨ GPU ä¸Šè®­ç»ƒ
#+begin_src jupyter-python
class CNN(nn.Module):
    def __init__(self) -> None:
        super(CNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(Xtrain_nn.shape[1], 20, 3,padding=2),
            nn.Tanh(),
            nn.AvgPool1d(3),
        )
        self.fc = nn.Sequential(
            nn.Linear(20, len(encode)),
            nn.ReLU(),
            nn.Softmax(dim=1),
        )

    def forward(self, x):
        out = self.conv(x)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out


net = CNN()
optimizer = torch.optim.Adamax(net.parameters(),lr=0.0025)
loss_func = torch.nn.L1Loss()
epochnum = 10000
for epoch in range(epochnum):
    prediction = net(Xtrain_nn.unsqueeze(2))
    loss = loss_func(Ytrain_nn, prediction)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % (epochnum / 10) == 0:
        print("epoch:", epoch, "loss:", loss.item())
prediction = pd.DataFrame(net(Xtest_nn.unsqueeze(2)).detach().numpy())
Ypredict = prediction.idxmax(axis=1).map(lambda x: encode[x])
result["CNN"] = get_score(Xtest, Ytest, lambda _: Ypredict)
result["CNN"]
result
#+end_src

å¢åŠ ç½‘ç»œå±‚æ•°å¯èƒ½ä¼šå¯¼è‡´æ¢¯åº¦ç¦»æ•£å’Œæ¢¯åº¦çˆ†ç‚¸çš„æƒ…å†µï¼Œåè€Œæ•ˆæœä¸å¥½ã€‚æ®‹å·®ç½‘ç»œ ResNet åˆ©ç”¨åœ¨ç½‘ç»œé—´åŠ å…¥ shortcut ï¼Œä½¿æ›´æ·±å±‚æ¬¡çš„è®­ç»ƒç»“æœè‡³å°‘ä¸å·®äºæ›´æµ…å±‚æ¬¡ï¼ˆå¦‚æœæ›´å·®å°±ç›´æ¥èµ° shortcut ï¼‰
*** RNN
[[https://pic1.zhimg.com/80/v2-ea6d9bcb018d897518a8f076e7f9fdcc_1440w.jpg]]
å¾ªç¯ç¥ç»ç½‘ç»œï¼šå¸¸ç”¨åœ¨ NLP ä¸­å¹¶å¤§æ”¾å¼‚å½©ï¼Œä¹Ÿä¼šåº”ç”¨åœ¨è‚¡ä»·ç­‰æ—¶é—´åºåˆ—ä¸­ã€‚ä»–ä¼šçŸ­æœŸåœ°â€œè®°ä½â€å‚æ•°ï¼Œå°±å¦‚åŒæˆ‘è¯´è¿™å¥è¯çš„æ—¶å€™ä½ çŸ­æœŸåœ°è®°ä½äº†ä¸Šä¸€å¥è¯ï¼Œä¼šæ›´æ–°â€œè‡ªæˆ‘â€è€Œéç›´æ¥å‘å‰ä¼ é€’ï¼Œåœ¨è¯¥å±‚ä¸­â€œå¾ªç¯â€ã€‚å³å¯¹äºéšè—å±‚è€Œè¨€ï¼Œ\(h_t = f_w(h_{t-1}, x_t)\) ã€‚éšç€è¾“å…¥çš„æ›´æ–°ï¼Œæœ‰ä¸€ä¸ªçŸ­æš‚çš„ memory ï¼Œè®°ä½åˆšåˆšçš„å‚æ•°ã€‚
#+begin_src jupyter-python
class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()
        self.rnn = nn.RNN(
            input_size=48,
            hidden_size=100,
            batch_first=True,
            # bidirectional=True,
        )
        self.fc = nn.Sequential(
            nn.Linear(100, len(encode)),
            nn.ReLU(),
            nn.Softmax(dim=1),
        )

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out

net = RNN()
optimizer = torch.optim.Adamax(net.parameters())
loss_func = nn.MSELoss()
epochnum = 3000
for epoch in range(epochnum):
    out = net(Xtrain_nn.unsqueeze(1))
    loss = loss_func(out, Ytrain_nn)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % (epochnum / 10) == 0:
        print("epoch:", epoch, "loss:", loss.item())

prediction = pd.DataFrame(net(Xtest_nn.unsqueeze(1)).detach().numpy())
Ypredict = prediction.idxmax(axis=1).map(lambda x: encode[x])
result["RNN"] = get_score(Xtest, Ytest, lambda _: Ypredict)
result["RNN"]
#+end_src

ä½†æ˜¯ RNN çš„çš„æ¢¯åº¦éå¸¸å®¹æ˜“â€œçˆ†ç‚¸â€ï¼ˆç‰¹åˆ«å¤§ï¼‰æˆ–â€œç¦»æ•£â€ï¼ˆç‰¹åˆ«å°ä»¥è‡´äºä¸æ›´æ–°ï¼‰ï¼Œé¢„æµ‹å¯èƒ½ä¼šå‡ºé”™ã€‚
é’ˆå¯¹æ­¤ï¼ŒLSTM ï¼ˆLong Short Term Memoryï¼‰æ¨¡å‹è®¾è®¡äº†ä¸‰ä¸ªâ€œé—¨â€ï¼šè¾“å…¥é—¨ =i= ï¼Œé—å¿˜é—¨ =f= ï¼Œè¾“å‡ºé—¨ =o= ï¼Œæœ‰ä¸€ç¯‡éå¸¸å¥½çš„[[https://colah.github.io/posts/2015-08-Understanding-LSTMs/][blog]]è¯¦ç»†æè¿°äº†è¿™äº›é—¨æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œç®€è€Œè¨€ä¹‹ä»–åŠ å…¥äº†é•¿æœŸè®°å¿†çš„éƒ¨åˆ†ã€‚

*** GAN & RL

+ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼šéšæœºå–æ ·ä½œä¸ºè¾“å…¥ï¼Œå…¶è¾“å‡ºç»“æœéœ€è¦å°½é‡æ¨¡ä»¿è®­ç»ƒé›†ä¸­çš„çœŸå®æ ·æœ¬ï¼Œä½¿åˆ¤åˆ«ç½‘ç»œæ— æ³•åˆ¤æ–­ç”Ÿæˆç½‘ç»œçš„è¾“å‡ºç»“æœæ˜¯å¦çœŸå®
+ å¼ºåŒ–å­¦ä¹ ï¼šåšå¼ˆè®ºâ€¦â€¦
#+begin_quote
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé¢†åŸŸï¼Œæ¶‰åŠè½¯ä»¶ä»£ç†å¦‚ä½•åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–ä¸€äº›ç´¯ç§¯å¥–åŠ±çš„æ¦‚å¿µã€‚è¯¥é—®é¢˜ç”±äºå…¶ä¸€èˆ¬æ€§ï¼Œåœ¨è®¸å¤šå…¶ä»–å­¦ç§‘ä¸­å¾—åˆ°ç ”ç©¶ï¼Œå¦‚åšå¼ˆè®ºï¼Œæ§åˆ¶ç†è®ºï¼Œè¿ç­¹å­¦ï¼Œä¿¡æ¯è®ºï¼ŒåŸºäºä»¿çœŸçš„ä¼˜åŒ–ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç¾¤æ™ºèƒ½ï¼Œç»Ÿè®¡å’Œé—ä¼ ç®—æ³•ã€‚ã€‚åœ¨è¿ç­¹å­¦å’Œæ§åˆ¶æ–‡çŒ®ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ è¢«ç§°ä¸ºè¿‘ä¼¼åŠ¨æ€è§„åˆ’æˆ–ç¥ç»åŠ¨æ€è§„åˆ’ã€‚--Wikipedia
#+end_quote

** å¯¹æ¯”
#+begin_src jupyter-python
feature = ["precision", "recall", "f1", "\(R^2\)"]
[["model"]+feature]+list([i[0]]+ [round(j,4) for j in i[1].values()] for i in result.items())
#+end_src

#+RESULTS:
| model             | precision | recall |     f1 | \(R^2\) |
| random            |    0.2364 | 0.1255 | 0.1544 |  0.0089 |
| logit             |    0.1815 | 0.2441 | 0.1547 | -0.0178 |
| decision tree     |    0.3499 | 0.3799 | 0.3529 |  0.3632 |
| random forest     |     0.396 | 0.4252 | 0.3835 |  0.3996 |
| gradient boosting |    0.5305 | 0.5256 | 0.5095 |  0.5421 |
| svm               |    0.4137 | 0.4094 | 0.3517 |  0.3431 |
| KNN               |    0.3625 | 0.3524 |  0.342 |  0.2987 |
| bp neural network |    0.2739 | 0.3386 | 0.2763 | -0.0591 |
| CNN               |    0.4308 | 0.4724 |  0.445 |  0.4383 |
| RNN               |    0.4046 | 0.4508 | 0.4241 |  0.4729 |

#+begin_src jupyter-python
N = len(feature)
angles = np.linspace(0, 2 * np.pi, N, endpoint=False)
angles = np.concatenate((angles, [angles[0]]))
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)
for model in result:
    values = [i for i in result[model].values()] + [result[model]["precision"]]
    ax.plot(angles, values, label=model)
    ax.fill(angles, values, alpha=0.1)
ax.set_thetagrids(angles[:-1] * 180 / np.pi, feature)
ax.grid(True)
plt.legend(bbox_to_anchor=(1.2, -0.1), ncol=3)
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: [0;31m---------------------------------------------------------------------------[0m
: [0;31mNameError[0m                                 Traceback (most recent call last)
: Input [0;32mIn [5][0m, in [0;36m<cell line: 1>[0;34m()[0m
: [0;32m----> 1[0m N [38;5;241m=[39m [38;5;28mlen[39m([43mfeature[49m)
: [1;32m      2[0m angles [38;5;241m=[39m np[38;5;241m.[39mlinspace([38;5;241m0[39m, [38;5;241m2[39m [38;5;241m*[39m np[38;5;241m.[39mpi, N, endpoint[38;5;241m=[39m[38;5;28;01mFalse[39;00m)
: [1;32m      3[0m angles [38;5;241m=[39m np[38;5;241m.[39mconcatenate((angles, [angles[[38;5;241m0[39m]]))
:
: [0;31mNameError[0m: name 'feature' is not defined
:END:


#+print_bibliography:
